"""
IELTS Scoring API - Main Application
=====================================
API for scoring IELTS Writing and Speaking

Endpoints:
- POST /api/writing/score - Score IELTS Writing Task 2
- POST /api/speaking/score-text - Score Speaking from transcript
- POST /api/speaking/score-audio - Score Speaking from audio file (Whisper ASR)

Run:
    uvicorn app:app --reload --host 0.0.0.0 --port 8000
"""

import os
import logging
from contextlib import asynccontextmanager
from typing import Optional

from fastapi import FastAPI, File, UploadFile, HTTPException, Form, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
import torch
import numpy as np
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# ======================== CONFIG ========================
# Láº¥y Ä‘Æ°á»ng dáº«n gá»‘c tá»« biáº¿n mÃ´i trÆ°á»ng (máº·c Ä‘á»‹nh /models cho Docker, hoáº·c "." cho local)
BASE_MODEL_DIR = os.getenv("MODEL_DIR", ".")

# XÃ¢y dá»±ng Ä‘Æ°á»ng dáº«n tuyá»‡t Ä‘á»‘i
WRITING_MODEL_DIR = os.path.join(BASE_MODEL_DIR, "ielts-writing-v3-classification")  # Classification model (12 classes)
SPEAKING_MODEL_DIR = os.path.join(BASE_MODEL_DIR, "speaking-cefr-roberta")  # New model trained on ICNALE + CEFR-Explorer
ENABLE_WHISPER = True  # Set to False to disable Whisper (saves memory)

# Writing model band classes (for classification)
WRITING_BAND_CLASSES = [3.5, 4.0, 4.5, 5.0, 5.5, 6.0, 6.5, 7.0, 7.5, 8.0, 8.5, 9.0]
WRITING_IDX_TO_BAND = {i: band for i, band in enumerate(WRITING_BAND_CLASSES)}

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Speaking constants
CEFR_LABELS = ["A1", "A2", "B1", "B2", "C1", "C2"]
ID2LABEL = {i: lbl for i, lbl in enumerate(CEFR_LABELS)}
CEFR_TO_IELTS = {
    "A1": 2.5, "A2": 3.5, "B1": 5.0,
    "B2": 6.5, "C1": 7.5, "C2": 8.5
}


# ======================== MODEL MANAGERS (Best Practice: Lazy Loading) ========================
class ModelManager:
    """Singleton model manager for lazy loading and caching models."""
    
    _writing_model = None
    _writing_tokenizer = None
    _speaking_model = None
    _speaking_tokenizer = None
    _whisper_service = None
    _speaking_available = None  # None = not checked, True/False = result
    
    @classmethod
    def get_writing_model(cls):
        if cls._writing_model is None:
            logger.info("ðŸ“š Loading Writing Model...")
            cls._writing_tokenizer = AutoTokenizer.from_pretrained(WRITING_MODEL_DIR)
            cls._writing_model = AutoModelForSequenceClassification.from_pretrained(
                WRITING_MODEL_DIR
            ).to(device)
            cls._writing_model.eval()
            logger.info("âœ… Writing Model loaded!")
        return cls._writing_model, cls._writing_tokenizer
    
    @classmethod
    def get_speaking_model(cls):
        if cls._speaking_available is None:
            # Check if model exists
            cls._speaking_available = os.path.exists(SPEAKING_MODEL_DIR)
        
        if not cls._speaking_available:
            raise FileNotFoundError(
                f"Speaking model not found at '{SPEAKING_MODEL_DIR}'. "
                f"Train it first using: python train_speaking_level.py"
            )
        
        if cls._speaking_model is None:
            logger.info("ðŸ“š Loading Speaking Model...")
            cls._speaking_tokenizer = AutoTokenizer.from_pretrained(SPEAKING_MODEL_DIR)
            cls._speaking_model = AutoModelForSequenceClassification.from_pretrained(
                SPEAKING_MODEL_DIR
            ).to(device)
            cls._speaking_model.eval()
            logger.info("âœ… Speaking Model loaded!")
        return cls._speaking_model, cls._speaking_tokenizer
    
    @classmethod
    def is_speaking_available(cls):
        if cls._speaking_available is None:
            cls._speaking_available = os.path.exists(SPEAKING_MODEL_DIR)
        return cls._speaking_available
    
    @classmethod
    def get_whisper_service(cls):
        if cls._whisper_service is None and ENABLE_WHISPER:
            try:
                from services.speech_service import whisper_manager, preload_model
                logger.info("ðŸŽ¤ Loading Whisper Model...")
                preload_model()
                cls._whisper_service = whisper_manager
                logger.info("âœ… Whisper Model loaded!")
            except ImportError as e:
                logger.warning(f"âš ï¸ Whisper not available: {e}")
                cls._whisper_service = None
        return cls._whisper_service


# ======================== LIFESPAN (Best Practice: Startup/Shutdown) ========================
@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    Lifespan context manager for startup and shutdown events.
    Preload models on startup for faster first request.
    """
    logger.info("ðŸš€ Starting IELTS Scoring API...")
    logger.info(f"ðŸ“Š Device: {device}")
    
    # Preload models on startup
    ModelManager.get_writing_model()
    
    # Speaking model (optional - may not exist)
    if ModelManager.is_speaking_available():
        ModelManager.get_speaking_model()
    else:
        logger.warning("âš ï¸ Speaking model not found. Train it using: python train_speaking_level.py")
    
    if ENABLE_WHISPER:
        ModelManager.get_whisper_service()
    
    logger.info("âœ… API ready! (Some models may be unavailable)")
    
    yield  # Server is running
    
    # Cleanup on shutdown
    logger.info("ðŸ‘‹ Shutting down IELTS Scoring API...")


# ======================== FASTAPI APP ========================
app = FastAPI(
    title="IELTS Scoring API",
    description="""
    ðŸŽ¯ **IELTS Scoring API** - AI-powered scoring for Writing and Speaking

    ## Features
    - âœï¸ **Writing Scoring**: Score IELTS Task 2 essays (Band 0-9)
    - ðŸŽ¤ **Speaking Scoring**: Score speaking from text or audio
    - ðŸŽ™ï¸ **Speech-to-Text**: Whisper-powered transcription

    ## Models
    - Writing: Fine-tuned RoBERTa on IELTS essays
    - Speaking: Fine-tuned DistilRoBERTa for CEFR classification
    - ASR: OpenAI Whisper (base model)
    """,
    version="2.0.0",
    docs_url="/docs",
    redoc_url="/redoc",
    lifespan=lifespan
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Production: replace with specific domains
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ======================== REQUEST/RESPONSE MODELS ========================
class WritingRequest(BaseModel):
    prompt: Optional[str] = Field(None, description="Essay prompt/question (optional)")
    essay: str = Field(..., min_length=50, description="Essay text to score")

    model_config = {
        "json_schema_extra": {
            "examples": [{
                "prompt": "Some people believe technology has made our lives more complicated. To what extent do you agree?",
                "essay": "In today's rapidly evolving world, technology has become an integral part of our daily lives. While some argue that it has simplified many aspects of life, others believe it has added unnecessary complexity. This essay will discuss both perspectives before presenting my own view..."
            }]
        }
    }


class WritingResponse(BaseModel):
    overall_band: float = Field(..., ge=3.5, le=9, description="IELTS band score (3.5-9.0)")
    confidence: float = Field(..., ge=0, le=1, description="Model confidence in prediction")
    top_predictions: list = Field(..., description="Top 3 predictions with probabilities")
    feedback: dict = Field(..., description="Feedback for 4 criteria")


class SpeakingTextRequest(BaseModel):
    answer_text: str = Field(..., min_length=10, description="Speaking answer transcript")

    model_config = {
        "json_schema_extra": {
            "examples": [{
                "answer_text": "Well, I think technology has changed our lives in many ways. For example, smartphones allow us to communicate with people around the world instantly. Also, we can access information very quickly through the internet."
            }]
        }
    }


class SpeakingTextResponse(BaseModel):
    cefr_level: str = Field(..., description="CEFR level (A1-C2)")
    approx_ielts_band: float = Field(..., description="Approximate IELTS band")
    feedback: dict = Field(..., description="Feedback for 4 criteria")


class SpeakingAudioResponse(BaseModel):
    transcript: str = Field(..., description="Transcribed text from audio")
    transcript_info: dict = Field(..., description="Transcription metadata")
    cefr_level: str = Field(..., description="CEFR level (A1-C2)")
    approx_ielts_band: float = Field(..., description="Approximate IELTS band")
    feedback: dict = Field(..., description="Feedback for 4 criteria")


class TranscribeResponse(BaseModel):
    text: str = Field(..., description="Transcribed text")
    language: str = Field(..., description="Detected language")
    duration_seconds: float = Field(..., description="Audio duration")
    word_count: int = Field(..., description="Number of words")


class ErrorResponse(BaseModel):
    error: str
    detail: str


# ======================== PREDICTION FUNCTIONS ========================
def predict_writing_band(essay: str) -> dict:
    """
    Predict IELTS Writing band score using RoBERTa Classification model.
    
    Model trained on IELTS Writing Task 2 essays dataset.
    Performance: 37.7% exact, 70.6% within Â±0.5, 87.4% within Â±1.0
    
    Returns:
        dict with 'band', 'confidence', and 'top_predictions'
    """
    model, tokenizer = ModelManager.get_writing_model()
    
    enc = tokenizer(
        essay,
        return_tensors="pt",
        truncation=True,
        max_length=512,
        padding="max_length",
    ).to(device)

    with torch.no_grad():
        outputs = model(**enc)
        logits = outputs.logits.detach().cpu().numpy()[0]
        
        # Apply softmax for probabilities
        exp_logits = np.exp(logits - np.max(logits))
        probs = exp_logits / exp_logits.sum()
        
        # Get predicted class
        pred_idx = int(np.argmax(probs))
        confidence = float(probs[pred_idx])
        
        # Get top 3 predictions
        top_indices = np.argsort(probs)[::-1][:3]
        top_predictions = [
            {"band": WRITING_IDX_TO_BAND[idx], "probability": float(probs[idx])}
            for idx in top_indices
        ]
    
    return {
        "band": WRITING_IDX_TO_BAND[pred_idx],
        "confidence": confidence,
        "top_predictions": top_predictions
    }


def predict_cefr_and_band(text: str) -> tuple[str, float]:
    """Predict CEFR level (A1-C2) and approximate IELTS Speaking band"""
    model, tokenizer = ModelManager.get_speaking_model()
    
    enc = tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        max_length=128,
        padding="max_length",
    ).to(device)

    with torch.no_grad():
        outputs = model(**enc)
        logits = outputs.logits.detach().cpu().numpy()
        pred_id = int(np.argmax(logits, axis=-1)[0])

    cefr = ID2LABEL[pred_id]
    band = CEFR_TO_IELTS.get(cefr, 0.0)
    return cefr, band


# ======================== FEEDBACK BUILDERS ========================
def build_writing_feedback(band: float) -> dict:
    """Generate rule-based feedback for Writing based on band score"""
    if band < 5.0:
        return {
            "task_response": "BÃ i viáº¿t chÆ°a tráº£ lá»i Ä‘áº§y Ä‘á»§ yÃªu cáº§u Ä‘á» bÃ i. HÃ£y táº­p trung hiá»ƒu rÃµ cÃ¢u há»i vÃ  Ä‘Æ°a ra cÃ¡c Ã½ chÃ­nh liÃªn quan.",
            "coherence_cohesion": "Cáº¥u trÃºc bÃ i cáº§n cáº£i thiá»‡n. Sá»­ dá»¥ng cÃ¡c Ä‘oáº¡n vÄƒn rÃµ rÃ ng vá»›i cÃ¢u chá»§ Ä‘á» vÃ  tá»« ná»‘i.",
            "vocabulary": "Vá»‘n tá»« vá»±ng cÃ²n háº¡n cháº¿. Cáº§n há»c thÃªm tá»« vá»±ng theo chá»§ Ä‘á» vÃ  cÃ¡c cá»¥m tá»« cá»‘ Ä‘á»‹nh (collocations).",
            "grammar": "Nhiá»u lá»—i ngá»¯ phÃ¡p áº£nh hÆ°á»Ÿng Ä‘áº¿n Ã½ nghÄ©a. Cáº§n luyá»‡n táº­p cÃ¡c cáº¥u trÃºc cÃ¢u cÆ¡ báº£n vÃ  cÃ¡c thÃ¬ phá»• biáº¿n."
        }
    elif band < 5.5:
        return {
            "task_response": "BÃ i viáº¿t Ä‘Ã£ Ä‘á» cáº­p Ä‘áº¿n yÃªu cáº§u Ä‘á» nhÆ°ng chÆ°a Ä‘áº§y Ä‘á»§. HÃ£y má»Ÿ rá»™ng Ã½ tÆ°á»Ÿng vá»›i cÃ¡c vÃ­ dá»¥ cá»¥ thá»ƒ vÃ  chi tiáº¿t hÆ¡n.",
            "coherence_cohesion": "Cáº¥u trÃºc bÃ i cáº§n cáº£i thiá»‡n. Má»—i Ä‘oáº¡n vÄƒn cáº§n cÃ³ cÃ¢u chá»§ Ä‘á» rÃµ rÃ ng vÃ  cÃ¡c cÃ¢u há»— trá»£. Sá»­ dá»¥ng Ä‘a dáº¡ng hÆ¡n cÃ¡c tá»« ná»‘i nhÆ° 'furthermore', 'however', 'consequently'.",
            "vocabulary": "Vá»‘n tá»« cÆ¡ báº£n, cÃ³ xu hÆ°á»›ng láº·p láº¡i. HÃ£y há»c thÃªm synonyms vÃ  cÃ¡c cá»¥m tá»« há»c thuáº­t nhÆ° 'it is widely believed that', 'there is a growing concern about'.",
            "grammar": "Lá»—i ngá»¯ phÃ¡p xuáº¥t hiá»‡n khÃ¡ thÆ°á»ng xuyÃªn. Cáº§n chÃº Ã½ Ä‘áº¿n subject-verb agreement, article usage, vÃ  cÃ¡c thÃ¬ Ä‘á»™ng tá»«."
        }
    elif band < 6.5:
        return {
            "task_response": "Báº¡n Ä‘Ã£ tráº£ lá»i Ä‘Æ°á»£c yÃªu cáº§u Ä‘á» bÃ i nhÆ°ng má»™t sá»‘ Ä‘iá»ƒm cÃ³ thá»ƒ phÃ¡t triá»ƒn thÃªm vá»›i vÃ­ dá»¥ cá»¥ thá»ƒ hÆ¡n.",
            "coherence_cohesion": "BÃ i viáº¿t cÃ³ tá»• chá»©c há»£p lÃ½ nhÆ°ng cÃ³ thá»ƒ cáº£i thiá»‡n cÃ¡ch chia Ä‘oáº¡n. Sá»­ dá»¥ng Ä‘a dáº¡ng hÆ¡n cÃ¡c tá»« ná»‘i vÃ  trÃ¡nh láº·p láº¡i 'firstly, secondly, thirdly'.",
            "vocabulary": "Vá»‘n tá»« Ä‘á»§ dÃ¹ng cho bÃ i viáº¿t. HÃ£y thá»­ dÃ¹ng tá»« ngá»¯ phá»©c táº¡p hÆ¡n nhÆ° collocations vÃ  idiomatic expressions.",
            "grammar": "Ngá»¯ phÃ¡p khÃ¡ tá»‘t vá»›i má»™t sá»‘ lá»—i nhá». Cáº§n luyá»‡n thÃªm cÃ¡c cáº¥u trÃºc cÃ¢u phá»©c táº¡p nhÆ° relative clauses, conditionals, vÃ  passive voice."
        }
    elif band < 7.5:
        return {
            "task_response": "BÃ i viáº¿t phÃ¡t triá»ƒn tá»‘t vá»›i quan Ä‘iá»ƒm rÃµ rÃ ng vÃ  cÃ¡c Ã½ tÆ°á»Ÿng má»Ÿ rá»™ng, liÃªn quan. Äá»ƒ Ä‘áº¡t band cao hÆ¡n, cáº§n cÃ³ phÃ¢n tÃ­ch sÃ¢u sáº¯c hÆ¡n.",
            "coherence_cohesion": "Tá»• chá»©c logic vá»›i viá»‡c sá»­ dá»¥ng hiá»‡u quáº£ cÃ¡c phÆ°Æ¡ng tiá»‡n liÃªn káº¿t. CÃ³ thá»ƒ cáº£i thiá»‡n báº±ng cÃ¡ch sá»­ dá»¥ng referencing pronouns vÃ  lexical cohesion.",
            "vocabulary": "Vá»‘n tá»« phong phÃº vÃ  Ä‘a dáº¡ng. Tiáº¿p tá»¥c má»Ÿ rá»™ng academic vocabulary vÃ  less common lexical items.",
            "grammar": "Sá»­ dá»¥ng Ä‘a dáº¡ng cáº¥u trÃºc ngá»¯ phÃ¡p vá»›i Ä‘á»™ chÃ­nh xÃ¡c cao. CÃ³ thá»ƒ thá»­ thÃªm inversions vÃ  cleft sentences."
        }
    else:
        return {
            "task_response": "BÃ i viáº¿t xuáº¥t sáº¯c vá»›i phÃ¢n tÃ­ch sÃ¢u sáº¯c vÃ  láº­p luáº­n cháº·t cháº½. Ã tÆ°á»Ÿng Ä‘Æ°á»£c phÃ¡t triá»ƒn Ä‘áº§y Ä‘á»§ vÃ  cÃ³ tÃ­nh thuyáº¿t phá»¥c.",
            "coherence_cohesion": "Tá»• chá»©c hoÃ n háº£o vá»›i sá»± chuyá»ƒn tiáº¿p mÆ°á»£t mÃ  giá»¯a cÃ¡c Ã½. Sá»­ dá»¥ng thÃ nh tháº¡o cÃ¡c phÆ°Æ¡ng tiá»‡n liÃªn káº¿t.",
            "vocabulary": "Vá»‘n tá»« phong phÃº, chÃ­nh xÃ¡c vÃ  tá»± nhiÃªn. Sá»­ dá»¥ng thÃ nh tháº¡o idioms, collocations vÃ  academic vocabulary.",
            "grammar": "Sá»­ dá»¥ng Ä‘a dáº¡ng vÃ  linh hoáº¡t cÃ¡c cáº¥u trÃºc ngá»¯ phÃ¡p phá»©c táº¡p vá»›i Ä‘á»™ chÃ­nh xÃ¡c gáº§n nhÆ° hoÃ n háº£o."
        }


def build_speaking_feedback(cefr: str, band: float) -> dict:
    """Generate rule-based feedback for Speaking based on CEFR level and band"""
    if band < 5.0:
        return {
            "fluency_coherence": "Tá»‘c Ä‘á»™ nÃ³i cÃ²n cháº­m vá»›i nhiá»u láº§n dá»«ng. HÃ£y luyá»‡n nÃ³i liÃªn tá»¥c hÆ¡n vá» cÃ¡c chá»§ Ä‘á» quen thuá»™c.",
            "vocabulary": "Vá»‘n tá»« cÃ²n háº¡n cháº¿. Cáº§n há»c thÃªm cÃ¡c cá»¥m diá»…n Ä‘áº¡t hÃ ng ngÃ y vÃ  tá»« vá»±ng theo chá»§ Ä‘á».",
            "grammar": "CÃ²n nhiá»u lá»—i ngá»¯ phÃ¡p cÆ¡ báº£n. Táº­p trung luyá»‡n cÃ¡c thÃ¬ hiá»‡n táº¡i, quÃ¡ khá»© vÃ  tÆ°Æ¡ng lai Ä‘Æ¡n.",
            "pronunciation": "PhÃ¡t Ã¢m cÃ³ thá»ƒ gÃ¢y khÃ³ hiá»ƒu cho ngÆ°á»i nghe. Luyá»‡n táº­p phÃ¡t Ã¢m tá»«ng Ã¢m vÃ  trá»ng Ã¢m tá»«."
        }
    elif band < 6.5:
        return {
            "fluency_coherence": "Báº¡n cÃ³ thá»ƒ duy trÃ¬ bÃ i nÃ³i vá» chá»§ Ä‘á» quen thuá»™c vá»›i má»™t chÃºt do dá»±. Sá»­ dá»¥ng thÃªm cÃ¡c tá»« ná»‘i.",
            "vocabulary": "Vá»‘n tá»« tá»‘t cho cÃ¡c chá»§ Ä‘á» quen thuá»™c. Má»Ÿ rá»™ng thÃªm cÃ¡c cá»¥m diá»…n Ä‘áº¡t vÃ  collocations.",
            "grammar": "Kiá»ƒm soÃ¡t tá»‘t cÃ¡c cáº¥u trÃºc Ä‘Æ¡n giáº£n. Luyá»‡n thÃªm cÃ¢u phá»©c vÃ  cÃ¢u Ä‘iá»u kiá»‡n.",
            "pronunciation": "PhÃ¡t Ã¢m khÃ¡ rÃµ rÃ ng. Cáº§n cáº£i thiá»‡n ngá»¯ Ä‘iá»‡u vÃ  cÃ¡ch ná»‘i Ã¢m."
        }
    else:
        return {
            "fluency_coherence": "Báº¡n nÃ³i trÃ´i cháº£y, chá»‰ thá»‰nh thoáº£ng do dá»±. Sá»­ dá»¥ng tuyá»‡t vá»i cÃ¡c tá»« ná»‘i.",
            "vocabulary": "Vá»‘n tá»« phong phÃº vá»›i viá»‡c sá»­ dá»¥ng tá»‘t thÃ nh ngá»¯ vÃ  collocations.",
            "grammar": "Kiá»ƒm soÃ¡t nháº¥t quÃ¡n cÃ¡c cáº¥u trÃºc phá»©c táº¡p, chá»‰ thá»‰nh thoáº£ng cÃ³ lá»—i nhá».",
            "pronunciation": "PhÃ¡t Ã¢m rÃµ rÃ ng, tá»± nhiÃªn vá»›i viá»‡c kiá»ƒm soÃ¡t tá»‘t trá»ng Ã¢m vÃ  ngá»¯ Ä‘iá»‡u."
        }


# ======================== EXCEPTION HANDLERS ========================
@app.exception_handler(Exception)
async def global_exception_handler(request, exc):
    logger.error(f"Unhandled error: {exc}")
    return JSONResponse(
        status_code=500,
        content={"error": "Internal Server Error", "detail": str(exc)}
    )


# ======================== API ENDPOINTS ========================

# ---------- Health & Info ----------
@app.get("/", tags=["Health"])
def root():
    """API root - health check and endpoint info"""
    return {
        "message": "ðŸŽ¯ IELTS Scoring API is running!",
        "version": "2.0.0",
        "endpoints": {
            "writing": "POST /api/writing/score",
            "speaking_text": "POST /api/speaking/score-text",
            "speaking_audio": "POST /api/speaking/score-audio",
            "transcribe": "POST /api/transcribe",
            "docs": "GET /docs"
        }
    }


@app.get("/health", tags=["Health"])
def health():
    """Detailed health check with model status"""
    whisper_status = False
    if ENABLE_WHISPER:
        try:
            from services.speech_service import get_whisper_info
            whisper_info = get_whisper_info()
            whisper_status = True
        except:
            whisper_info = {"error": "Whisper not available"}
    else:
        whisper_info = {"enabled": False}
    
    return {
        "status": "healthy",
        "device": str(device),
        "models": {
            "writing": True,
            "speaking": True,
            "whisper": whisper_status
        },
        "whisper_info": whisper_info
    }


# ---------- Writing Endpoints ----------
@app.post(
    "/api/writing/score",
    response_model=WritingResponse,
    tags=["Writing"],
    summary="Score IELTS Writing Task 2",
    responses={400: {"model": ErrorResponse}}
)
def score_writing(req: WritingRequest):
    """
    Score an IELTS Writing Task 2 essay.
    
    - **prompt**: (optional) The essay question
    - **essay**: The essay text (min 50 characters)
    
    Returns:
    - **overall_band**: Band score from 3.5 to 9.0
    - **confidence**: Model confidence in prediction
    - **feedback**: Feedback for Task Response, Coherence, Vocabulary, Grammar
    
    Model Performance:
    - Exact match: 37.7%
    - Within Â±0.5: 70.6%
    - Within Â±1.0: 87.4%
    """
    try:
        result = predict_writing_band(req.essay)
        band = result["band"]
        feedback = build_writing_feedback(band)
        return {
            "overall_band": band,
            "confidence": result["confidence"],
            "top_predictions": result["top_predictions"],
            "feedback": feedback
        }
    except Exception as e:
        logger.error(f"Writing scoring error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ---------- Speaking Endpoints ----------
@app.post(
    "/api/speaking/score-text",
    response_model=SpeakingTextResponse,
    tags=["Speaking"],
    summary="Score Speaking from transcript"
)
def score_speaking_text(req: SpeakingTextRequest):
    """
    Score IELTS Speaking from text transcript.
    
    - **answer_text**: The speaking answer transcript
    
    Returns:
    - **cefr_level**: CEFR level (A1-C2)
    - **approx_ielts_band**: Approximate IELTS band
    - **feedback**: Feedback for Fluency, Vocabulary, Grammar, Pronunciation
    """
    try:
        cefr, band = predict_cefr_and_band(req.answer_text)
        feedback = build_speaking_feedback(cefr, band)
        return {
            "cefr_level": cefr,
            "approx_ielts_band": band,
            "feedback": feedback
        }
    except Exception as e:
        logger.error(f"Speaking scoring error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post(
    "/api/speaking/score-audio",
    response_model=SpeakingAudioResponse,
    tags=["Speaking"],
    summary="Score Speaking from audio file",
    responses={
        400: {"model": ErrorResponse},
        503: {"description": "Whisper not available"}
    }
)
async def score_speaking_audio(
    audio: UploadFile = File(..., description="Audio file (mp3, wav, m4a, etc.)"),
    language: str = Query("en", description="Audio language code")
):
    """
    Score IELTS Speaking from audio file using Whisper ASR.
    
    1. Transcribes audio to text using Whisper
    2. Scores the transcript using CEFR model
    3. Returns transcript, CEFR level, IELTS band, and feedback
    
    **Supported formats**: mp3, wav, m4a, flac, ogg, webm, mp4  
    **Max file size**: 25MB  
    **Max duration**: 5 minutes
    """
    if not ENABLE_WHISPER:
        raise HTTPException(
            status_code=503,
            detail="Speech-to-text is disabled. Set ENABLE_WHISPER=True to enable."
        )
    
    try:
        from services.speech_service import (
            transcribe_audio_bytes,
            AudioValidationError
        )
        
        # Read audio file
        audio_bytes = await audio.read()
        
        if len(audio_bytes) == 0:
            raise HTTPException(status_code=400, detail="Empty audio file")
        
        # Transcribe
        result = await transcribe_audio_bytes(
            audio_bytes=audio_bytes,
            filename=audio.filename or "audio.wav",
            language=language,
            include_segments=False
        )
        
        transcript = result.text
        
        if not transcript or len(transcript.strip()) < 10:
            raise HTTPException(
                status_code=400,
                detail="Could not extract meaningful text from audio. Please ensure clear speech."
            )
        
        # Score the transcript
        cefr, band = predict_cefr_and_band(transcript)
        feedback = build_speaking_feedback(cefr, band)
        
        return {
            "transcript": transcript,
            "transcript_info": {
                "language": result.language,
                "duration_seconds": result.duration,
                "word_count": len(transcript.split()),
                "confidence": result.confidence
            },
            "cefr_level": cefr,
            "approx_ielts_band": band,
            "feedback": feedback
        }
        
    except AudioValidationError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Audio scoring error: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to process audio: {e}")


# ---------- Transcription Endpoint ----------
@app.post(
    "/api/transcribe",
    response_model=TranscribeResponse,
    tags=["Speech-to-Text"],
    summary="Transcribe audio to text",
    responses={
        400: {"model": ErrorResponse},
        503: {"description": "Whisper not available"}
    }
)
async def transcribe_audio(
    audio: UploadFile = File(..., description="Audio file to transcribe"),
    language: str = Query("en", description="Audio language code (e.g., 'en', 'vi')")
):
    """
    Transcribe audio file to text using Whisper ASR.
    
    **Supported formats**: mp3, wav, m4a, flac, ogg, webm, mp4  
    **Max file size**: 25MB  
    **Max duration**: 5 minutes
    """
    if not ENABLE_WHISPER:
        raise HTTPException(
            status_code=503,
            detail="Speech-to-text is disabled"
        )
    
    try:
        from services.speech_service import (
            transcribe_audio_bytes,
            AudioValidationError
        )
        
        audio_bytes = await audio.read()
        
        result = await transcribe_audio_bytes(
            audio_bytes=audio_bytes,
            filename=audio.filename or "audio.wav",
            language=language
        )
        
        return {
            "text": result.text,
            "language": result.language,
            "duration_seconds": result.duration,
            "word_count": len(result.text.split())
        }
        
    except AudioValidationError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.error(f"Transcription error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ======================== RUN SERVER ========================
if __name__ == "__main__":
    import uvicorn
    
    print("\n" + "="*60)
    print("ðŸš€ IELTS Scoring API Server")
    print("="*60)
    print(f"ðŸ“Š Device: {device}")
    print(f"ðŸŽ¤ Whisper: {'Enabled' if ENABLE_WHISPER else 'Disabled'}")
    print("\nðŸ“– API Docs: http://localhost:8000/docs")
    print("="*60 + "\n")
    
    uvicorn.run(
        "app:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )
