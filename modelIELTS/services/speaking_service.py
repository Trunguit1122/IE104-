"""
Speaking Service Module
=======================
Standalone service for IELTS Speaking scoring (from text transcript).
Uses CEFR level classification and maps to IELTS band.

Model: RoBERTa-base fine-tuned on ICNALE + CEFR-Explorer (3,200+ samples)
Performance: 87.1% exact accuracy, 99.4% within 1 CEFR level

NOTE: Model is loaded lazily on first use.
      Train the model first using train_speaking_level.py
"""

import os
import torch
import numpy as np

MODEL_DIR = "./speaking-cefr-roberta"

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# CEFR labels and mapping
CEFR_LABELS = ["A1", "A2", "B1", "B2", "C1", "C2"]
ID2LABEL = {i: lbl for i, lbl in enumerate(CEFR_LABELS)}
CEFR_TO_IELTS = {
    "A1": 2.5,
    "A2": 3.5,
    "B1": 5.0,
    "B2": 6.5,
    "C1": 7.5,
    "C2": 8.5,
}

# Lazy loading - model and tokenizer
_model = None
_tokenizer = None


def _load_model():
    """Lazy load model and tokenizer on first use."""
    global _model, _tokenizer
    
    if _model is not None:
        return _model, _tokenizer
    
    # Check if model exists
    if not os.path.exists(MODEL_DIR):
        raise FileNotFoundError(
            f"Speaking model not found at '{MODEL_DIR}'.\n"
            f"Please train the model first using: python train_speaking_level.py"
        )
    
    from transformers import AutoTokenizer, AutoModelForSequenceClassification
    
    print(f"ðŸ“ Loading Speaking Model from {MODEL_DIR}...")
    _tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)
    _model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR).to(device)
    _model.eval()
    print(f"   âœ… Model loaded successfully on {device}")
    
    return _model, _tokenizer


def predict_cefr_and_band(text: str) -> tuple[str, float]:
    """
    Predict CEFR level and approximate IELTS Speaking band.
    
    Args:
        text: The speaking transcript to score
        
    Returns:
        Tuple of (cefr_level, ielts_band)
    """
    model, tokenizer = _load_model()
    
    enc = tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        max_length=128,
        padding="max_length",
    ).to(device)

    with torch.no_grad():
        outputs = model(**enc)
        logits = outputs.logits.detach().cpu().numpy()
        pred_id = int(np.argmax(logits, axis=-1)[0])

    cefr = ID2LABEL[pred_id]
    band = CEFR_TO_IELTS.get(cefr, 0.0)
    return cefr, band


def get_cefr_probabilities(text: str) -> dict:
    """
    Get probability distribution over CEFR levels.
    
    Args:
        text: The speaking transcript
        
    Returns:
        Dictionary mapping CEFR levels to probabilities
    """
    model, tokenizer = _load_model()
    
    enc = tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        max_length=128,
        padding="max_length",
    ).to(device)

    with torch.no_grad():
        outputs = model(**enc)
        logits = outputs.logits.detach().cpu().numpy()[0]
        # Apply softmax
        exp_logits = np.exp(logits - np.max(logits))
        probs = exp_logits / exp_logits.sum()

    return {CEFR_LABELS[i]: float(probs[i]) for i in range(len(CEFR_LABELS))}


def build_speaking_feedback(cefr: str, band: float) -> dict:
    """
    Generate feedback based on CEFR level and band.
    
    Args:
        cefr: The predicted CEFR level
        band: The approximate IELTS band
        
    Returns:
        Dictionary with feedback for 4 Speaking criteria
    """
    if band < 5.0:
        return {
            "fluency_coherence": "Tá»‘c Ä‘á»™ nÃ³i cÃ²n cháº­m vá»›i nhiá»u láº§n dá»«ng. HÃ£y luyá»‡n nÃ³i liÃªn tá»¥c hÆ¡n vá» cÃ¡c chá»§ Ä‘á» quen thuá»™c.",
            "vocabulary": "Vá»‘n tá»« cÃ²n háº¡n cháº¿. Cáº§n há»c thÃªm cÃ¡c cá»¥m diá»…n Ä‘áº¡t hÃ ng ngÃ y vÃ  tá»« vá»±ng theo chá»§ Ä‘á».",
            "grammar": "CÃ²n nhiá»u lá»—i ngá»¯ phÃ¡p cÆ¡ báº£n. Táº­p trung luyá»‡n cÃ¡c thÃ¬ hiá»‡n táº¡i, quÃ¡ khá»© vÃ  tÆ°Æ¡ng lai Ä‘Æ¡n.",
            "pronunciation": "PhÃ¡t Ã¢m cÃ³ thá»ƒ gÃ¢y khÃ³ hiá»ƒu cho ngÆ°á»i nghe. Luyá»‡n táº­p phÃ¡t Ã¢m tá»«ng Ã¢m vÃ  trá»ng Ã¢m tá»«."
        }
    elif band < 6.5:
        return {
            "fluency_coherence": "Báº¡n cÃ³ thá»ƒ duy trÃ¬ bÃ i nÃ³i vá» chá»§ Ä‘á» quen thuá»™c vá»›i má»™t chÃºt do dá»±. Sá»­ dá»¥ng thÃªm cÃ¡c tá»« ná»‘i.",
            "vocabulary": "Vá»‘n tá»« tá»‘t cho cÃ¡c chá»§ Ä‘á» quen thuá»™c. Má»Ÿ rá»™ng thÃªm cÃ¡c cá»¥m diá»…n Ä‘áº¡t vÃ  collocations.",
            "grammar": "Kiá»ƒm soÃ¡t tá»‘t cÃ¡c cáº¥u trÃºc Ä‘Æ¡n giáº£n. Luyá»‡n thÃªm cÃ¢u phá»©c vÃ  cÃ¢u Ä‘iá»u kiá»‡n.",
            "pronunciation": "PhÃ¡t Ã¢m khÃ¡ rÃµ rÃ ng. Cáº§n cáº£i thiá»‡n ngá»¯ Ä‘iá»‡u vÃ  cÃ¡ch ná»‘i Ã¢m."
        }
    else:
        return {
            "fluency_coherence": "Báº¡n nÃ³i trÃ´i cháº£y, chá»‰ thá»‰nh thoáº£ng do dá»±. Sá»­ dá»¥ng tuyá»‡t vá»i cÃ¡c tá»« ná»‘i.",
            "vocabulary": "Vá»‘n tá»« phong phÃº vá»›i viá»‡c sá»­ dá»¥ng tá»‘t thÃ nh ngá»¯ vÃ  collocations.",
            "grammar": "Kiá»ƒm soÃ¡t nháº¥t quÃ¡n cÃ¡c cáº¥u trÃºc phá»©c táº¡p, chá»‰ thá»‰nh thoáº£ng cÃ³ lá»—i nhá».",
            "pronunciation": "PhÃ¡t Ã¢m rÃµ rÃ ng, tá»± nhiÃªn vá»›i viá»‡c kiá»ƒm soÃ¡t tá»‘t trá»ng Ã¢m vÃ  ngá»¯ Ä‘iá»‡u."
        }


def score_speaking(text: str, include_probabilities: bool = False) -> dict:
    """
    Complete scoring function that returns CEFR, band, and feedback.
    
    Args:
        text: The speaking transcript to score
        include_probabilities: Whether to include CEFR probability distribution
        
    Returns:
        Dictionary with cefr_level, approx_ielts_band, feedback, and optionally probabilities
    """
    cefr, band = predict_cefr_and_band(text)
    feedback = build_speaking_feedback(cefr, band)
    
    result = {
        "cefr_level": cefr,
        "approx_ielts_band": band,
        "feedback": feedback
    }
    
    if include_probabilities:
        result["cefr_probabilities"] = get_cefr_probabilities(text)
    
    return result


# ============= CLI Testing =============
if __name__ == "__main__":
    print(f"Using device: {device}")
    
    sample_text = """
Well, I think technology has changed our lives in many significant ways,
both positive and negative. On the positive side, smartphones and the
internet have made communication incredibly easy. We can now connect with
friends and family anywhere in the world instantly through video calls or
messaging apps. Also, access to information has become much more convenient.

However, I also believe there are some drawbacks. Many people spend too
much time scrolling through social media instead of having meaningful
face-to-face conversations. This can lead to feelings of isolation and
anxiety, particularly among young people.

Overall, I think technology is a powerful tool that can enhance our lives
if we use it wisely and maintain a healthy balance.
    """.strip()

    print("\n" + "="*50)
    print("Testing Speaking Service")
    print("="*50)
    
    result = score_speaking(sample_text, include_probabilities=True)
    print(f"\nðŸ“Š CEFR Level: {result['cefr_level']}")
    print(f"ðŸ“Š Approx IELTS Band: {result['approx_ielts_band']}")
    
    print("\nðŸ“Š CEFR Probabilities:")
    for level, prob in result['cefr_probabilities'].items():
        bar = "â–ˆ" * int(prob * 50)
        print(f"  {level}: {prob:.2%} {bar}")
    
    print("\nðŸ“‹ Feedback:")
    for key, value in result['feedback'].items():
        print(f"  â€¢ {key}: {value}")
